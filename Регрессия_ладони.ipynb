{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Загрузка данных"
      ],
      "metadata": {
        "id": "i0Uz6R3jZ_iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/iurii-chiryshev/PalmEdu\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown \"https://drive.google.com/uc?id=193YTyFo8QuQsAMvi8oxNhNhicFSNmuW8\"\n",
        "!unzip \"data.zip\""
      ],
      "metadata": {
        "id": "b7v-gwadG2on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Решение пошло"
      ],
      "metadata": {
        "id": "U-2OI2tuaE0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "from collections import OrderedDict\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import random\n",
        "import math\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "MBFeV4TLaKBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Description for the script:\n",
        "SSR-Net in Pytorch.\n",
        "@see https://raw.githubusercontent.com/oukohou/SSR_Net_Pytorch/master/SSR_models/SSR_Net_model.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "class SSRNet(nn.Module):\n",
        "    def __init__(self, stage_num=[3, 3, 3], image_size=64,\n",
        "                 class_range=101, lambda_index=1., lambda_delta=1.):\n",
        "        super(SSRNet, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.stage_num = stage_num\n",
        "        self.lambda_index = lambda_index\n",
        "        self.lambda_delta = lambda_delta\n",
        "        self.class_range = class_range\n",
        "\n",
        "        self.stream1_stage3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2, 2)\n",
        "        )\n",
        "        self.stream1_stage2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2, 2)\n",
        "        )\n",
        "        self.stream1_stage1 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(2, 2),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            # nn.AvgPool2d(2, 2) # paper has this layer, but official codes don't.\n",
        "        )\n",
        "\n",
        "        self.stream2_stage3 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.stream2_stage2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.stream2_stage1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Tanh(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(16, 16, 3, 1, 1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Tanh(),\n",
        "            # nn.MaxPool2d(2, 2) # paper has this layer, but official codes don't.\n",
        "        )\n",
        "\n",
        "        # fusion block\n",
        "        self.funsion_block_stream1_stage_3_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(32, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(8, 8)\n",
        "        )\n",
        "        self.funsion_block_stream1_stage_3_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 4 * 4, self.stage_num[2]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.funsion_block_stream1_stage_2_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(32, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(4, 4)\n",
        "        )\n",
        "        self.funsion_block_stream1_stage_2_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 4 * 4, self.stage_num[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.funsion_block_stream1_stage_1_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(32, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            # nn.AvgPool2d(2, 2) # paper has this layer, but official codes don't.\n",
        "        )\n",
        "        self.funsion_block_stream1_stage_1_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 8 * 8, self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # stream2\n",
        "        self.funsion_block_stream2_stage_3_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(8, 8)\n",
        "        )\n",
        "        self.funsion_block_stream2_stage_3_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 4 * 4, self.stage_num[2]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.funsion_block_stream2_stage_2_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(4, 4)\n",
        "        )\n",
        "        self.funsion_block_stream2_stage_2_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 4 * 4, self.stage_num[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.funsion_block_stream2_stage_1_before_PB = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, 1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            # nn.MaxPool2d(2, 2) # paper has this layer, but official codes don't.\n",
        "        )\n",
        "        self.funsion_block_stream2_stage_1_prediction_block = nn.Sequential(\n",
        "            nn.Dropout(0.2, ),\n",
        "            nn.Linear(10 * 8 * 8, self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.stage3_FC_after_PB = nn.Sequential(\n",
        "            nn.Linear(self.stage_num[0], 2 * self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage3_prob = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage3_index_offsets = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.stage3_delta_k = nn.Sequential(\n",
        "            nn.Linear(10 * 4 * 4, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.stage2_FC_after_PB = nn.Sequential(\n",
        "            nn.Linear(self.stage_num[0], 2 * self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage2_prob = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage2_index_offsets = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.stage2_delta_k = nn.Sequential(\n",
        "            nn.Linear(10 * 4 * 4, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.stage1_FC_after_PB = nn.Sequential(\n",
        "            nn.Linear(self.stage_num[0], 2 * self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage1_prob = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.stage1_index_offsets = nn.Sequential(\n",
        "            nn.Linear(2 * self.stage_num[0], self.stage_num[0]),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.stage1_delta_k = nn.Sequential(\n",
        "            nn.Linear(10 * 8 * 8, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, image_):\n",
        "        feature_stream1_stage3 = self.stream1_stage3(image_)\n",
        "\n",
        "        feature_stream1_stage2 = self.stream1_stage2(feature_stream1_stage3)\n",
        "\n",
        "        feature_stream1_stage1 = self.stream1_stage1(feature_stream1_stage2)\n",
        "\n",
        "        feature_stream2_stage3 = self.stream2_stage3(image_)\n",
        "\n",
        "        feature_stream2_stage2 = self.stream2_stage2(feature_stream2_stage3)\n",
        "\n",
        "        feature_stream2_stage1 = self.stream2_stage1(feature_stream2_stage2)\n",
        "\n",
        "        feature_stream1_stage3_before_PB = self.funsion_block_stream1_stage_3_before_PB(feature_stream1_stage3)\n",
        "        feature_stream1_stage2_before_PB = self.funsion_block_stream1_stage_2_before_PB(feature_stream1_stage2)\n",
        "        feature_stream1_stage1_before_PB = self.funsion_block_stream1_stage_1_before_PB(feature_stream1_stage1)\n",
        "\n",
        "        feature_stream2_stage3_before_PB = self.funsion_block_stream2_stage_3_before_PB(feature_stream2_stage3)\n",
        "        feature_stream2_stage2_before_PB = self.funsion_block_stream2_stage_2_before_PB(feature_stream2_stage2)\n",
        "        feature_stream2_stage1_before_PB = self.funsion_block_stream2_stage_1_before_PB(feature_stream2_stage1)\n",
        "\n",
        "        embedding_stream1_stage3_before_PB = feature_stream1_stage3_before_PB.view(\n",
        "            feature_stream1_stage3_before_PB.size(0), -1)\n",
        "        embedding_stream1_stage2_before_PB = feature_stream1_stage2_before_PB.view(\n",
        "            feature_stream1_stage2_before_PB.size(0), -1)\n",
        "        embedding_stream1_stage1_before_PB = feature_stream1_stage1_before_PB.view(\n",
        "            feature_stream1_stage1_before_PB.size(0), -1)\n",
        "\n",
        "        embedding_stream2_stage3_before_PB = feature_stream2_stage3_before_PB.view(\n",
        "            feature_stream2_stage3_before_PB.size(0), -1)\n",
        "        embedding_stream2_stage2_before_PB = feature_stream2_stage2_before_PB.view(\n",
        "            feature_stream2_stage2_before_PB.size(0), -1)\n",
        "        embedding_stream2_stage1_before_PB = feature_stream2_stage1_before_PB.view(\n",
        "            feature_stream2_stage1_before_PB.size(0), -1)\n",
        "        stage1_delta_k = self.stage1_delta_k(\n",
        "            torch.mul(embedding_stream1_stage1_before_PB, embedding_stream2_stage1_before_PB))\n",
        "        stage2_delta_k = self.stage2_delta_k(\n",
        "            torch.mul(embedding_stream1_stage2_before_PB, embedding_stream2_stage2_before_PB))\n",
        "        stage3_delta_k = self.stage3_delta_k(\n",
        "            torch.mul(embedding_stream1_stage3_before_PB, embedding_stream2_stage3_before_PB))\n",
        "\n",
        "        embedding_stage1_after_PB = torch.mul(\n",
        "            self.funsion_block_stream1_stage_1_prediction_block(embedding_stream1_stage1_before_PB),\n",
        "            self.funsion_block_stream2_stage_1_prediction_block(embedding_stream2_stage1_before_PB))\n",
        "        embedding_stage2_after_PB = torch.mul(\n",
        "            self.funsion_block_stream1_stage_2_prediction_block(embedding_stream1_stage2_before_PB),\n",
        "            self.funsion_block_stream2_stage_2_prediction_block(embedding_stream2_stage2_before_PB))\n",
        "        embedding_stage3_after_PB = torch.mul(\n",
        "            self.funsion_block_stream1_stage_3_prediction_block(embedding_stream1_stage3_before_PB),\n",
        "            self.funsion_block_stream2_stage_3_prediction_block(embedding_stream2_stage3_before_PB))\n",
        "\n",
        "        embedding_stage1_after_PB = self.stage1_FC_after_PB(embedding_stage1_after_PB)\n",
        "        embedding_stage2_after_PB = self.stage2_FC_after_PB(embedding_stage2_after_PB)\n",
        "        embedding_stage3_after_PB = self.stage3_FC_after_PB(embedding_stage3_after_PB)\n",
        "\n",
        "        prob_stage_1 = self.stage1_prob(embedding_stage1_after_PB)\n",
        "        index_offset_stage1 = self.stage1_index_offsets(embedding_stage1_after_PB)\n",
        "\n",
        "        prob_stage_2 = self.stage2_prob(embedding_stage2_after_PB)\n",
        "        index_offset_stage2 = self.stage2_index_offsets(embedding_stage2_after_PB)\n",
        "\n",
        "        prob_stage_3 = self.stage3_prob(embedding_stage3_after_PB)\n",
        "        index_offset_stage3 = self.stage3_index_offsets(embedding_stage3_after_PB)\n",
        "        stage1_regress = prob_stage_1[:, 0] * 0\n",
        "        stage2_regress = prob_stage_2[:, 0] * 0\n",
        "        stage3_regress = prob_stage_3[:, 0] * 0\n",
        "        for index in range(self.stage_num[0]):\n",
        "            stage1_regress = stage1_regress + (\n",
        "                        index + self.lambda_index * index_offset_stage1[:, index]) * prob_stage_1[:, index]\n",
        "        stage1_regress = torch.unsqueeze(stage1_regress, 1)\n",
        "        stage1_regress = stage1_regress / (self.stage_num[0] * (1 + self.lambda_delta * stage1_delta_k))\n",
        "\n",
        "        for index in range(self.stage_num[1]):\n",
        "            stage2_regress = stage2_regress + (\n",
        "                        index + self.lambda_index * index_offset_stage2[:, index]) * prob_stage_2[:, index]\n",
        "        stage2_regress = torch.unsqueeze(stage2_regress, 1)\n",
        "        stage2_regress = stage2_regress / (self.stage_num[0] * (1 + self.lambda_delta * stage1_delta_k) *\n",
        "                                           (self.stage_num[1] * (1 + self.lambda_delta * stage2_delta_k)))\n",
        "\n",
        "        for index in range(self.stage_num[2]):\n",
        "            stage3_regress = stage3_regress + (\n",
        "                        index + self.lambda_index * index_offset_stage3[:, index]) * prob_stage_3[:, index]\n",
        "        stage3_regress = torch.unsqueeze(stage3_regress, 1)\n",
        "        stage3_regress = stage3_regress / (self.stage_num[0] * (1 + self.lambda_delta * stage1_delta_k) *\n",
        "                                           (self.stage_num[1] * (1 + self.lambda_delta * stage2_delta_k)) *\n",
        "                                           (self.stage_num[2] * (1 + self.lambda_delta * stage3_delta_k))\n",
        "                                           )\n",
        "        regress_class = (stage1_regress + stage2_regress + stage3_regress) * self.class_range\n",
        "        regress_class = torch.squeeze(regress_class, 1)\n",
        "        return regress_class\n"
      ],
      "metadata": {
        "id": "vmeIqVZOtKp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(3, 2).contiguous()\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class ONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ONet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            OrderedDict([\n",
        "                ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
        "                ('prelu1', nn.PReLU(32)),\n",
        "                ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "                ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
        "                ('prelu2', nn.PReLU(64)),\n",
        "                ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "                ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
        "                ('prelu3', nn.PReLU(64)),\n",
        "                ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
        "                ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
        "                ('prelu4', nn.PReLU(128)),\n",
        "            ]))\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.conv5 = nn.Linear(3200, 256)  # 21632 if [128x128], 3200 if [64x64]\n",
        "        self.drop5 = nn.Dropout(0.25)\n",
        "        self.prelu5 = nn.PReLU(256)\n",
        "        self.result = nn.Linear(256, 1) #\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.drop5(x)\n",
        "\n",
        "        x = self.result(x).view(-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "l_bd26t_IGkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(os.path.join(path,\"label.csv\"),sep=\";\")"
      ],
      "metadata": {
        "id": "vRgoZnnqt0-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AngleDatasets(data.Dataset):\n",
        "    def __init__(self, path, transforms=None):\n",
        "        self.path = path\n",
        "        self.transforms = transforms\n",
        "        df = pd.read_csv(os.path.join(path,\"label.csv\"),sep=\";\")\n",
        "        self.list = df.values.tolist()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.list[index]\n",
        "        file_name = row[0]\n",
        "        label = row[1]\n",
        "        img = cv2.imread(os.path.join(self.path, file_name),cv2.IMREAD_COLOR)\n",
        "\n",
        "        if self.transforms:\n",
        "            [img, label] = self.transforms(img,label)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list)\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, log_dir=None):\n",
        "        if TENSORBOARD:\n",
        "            self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        if TENSORBOARD:\n",
        "            with self.writer.as_default():\n",
        "                tf.summary.scalar(tag, value, step=step)\n",
        "                self.writer.flush()"
      ],
      "metadata": {
        "id": "uOfMTR8TZTbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import tensorflow as tf\n",
        "    TENSORBOARD = True\n",
        "except ImportError:\n",
        "    print('tensorflow not found')\n",
        "    TENSORBOARD = False"
      ],
      "metadata": {
        "id": "j2jtC9QsG2tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rotate(object):\n",
        "    def __init__(self, angle_range = (-45,45), prob = 0.5, padding_color=0):\n",
        "        \"\"\"\n",
        "        :param angle_range: Rotation angle in degrees. Positive values mean counter-clockwise rotation.\n",
        "        :param prob:\n",
        "        :param padding_color:\n",
        "        \"\"\"\n",
        "        self.arange = angle_range\n",
        "        self.prob = prob\n",
        "        self.padding_color = padding_color\n",
        "\n",
        "\n",
        "    def __call__(self, img, label):\n",
        "        if random.random() < self.prob:\n",
        "            # вращаем на случайный угол относительно центра изображения\n",
        "            (h, w, c) = img.shape\n",
        "            angle = random.random() * (self.arange[1] - self.arange[0]) + self.arange[0]\n",
        "            rot_mat = cv2.getRotationMatrix2D((w//2,h//2), angle, 1.0)\n",
        "            img = cv2.warpAffine(img, rot_mat, (w,h), flags=cv2.INTER_LINEAR,\n",
        "                                 borderMode=cv2.BORDER_CONSTANT,\n",
        "                                 borderValue = self.padding_color)\n",
        "            label = angle + label\n",
        "        return [img, label]\n",
        "\n",
        "class VeritcalFlip(object):\n",
        "    def __init__(self, prob = 0.5):\n",
        "        self.prob = prob\n",
        "    def __call__(self, image, label):\n",
        "        if random.random() < self.prob:\n",
        "            image = cv2.flip(image, 0) # veritcal flip\n",
        "            label = 360 - label\n",
        "        return [image, label]\n",
        "\n",
        "class Normalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "        image = image.astype(np.float32)\n",
        "        for i in range(3):\n",
        "            image[:,:,i] -= self.mean[i]\n",
        "        for i in range(3):\n",
        "            image[:,:, i] /= self.std[i]\n",
        "\n",
        "        return [image, label]\n",
        "\n",
        "class Resize(object):\n",
        "    def __init__(self, size=(64,64)):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "        image = cv2.resize(image, self.size)\n",
        "        return [image, label]\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, label):\n",
        "\n",
        "        image = image.transpose((2,0,1)) # change [H,W,C] to [C,H,W]\n",
        "\n",
        "        return [image, label]\n",
        "\n",
        "class Compose(object):\n",
        "    \"\"\"Composes several transforms together.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, *args):\n",
        "        for t in self.transforms:\n",
        "            args = t(*args)\n",
        "        return args"
      ],
      "metadata": {
        "id": "co4jZx8_G2wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    print('Save checkpoint to {0:}'.format(filename))\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, initial_lr, step_index):\n",
        "    lr = initial_lr * (0.1 ** (step_index))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "\n",
        "def train(net, trainloader, criterion, optimizer, device):\n",
        "    net.train()\n",
        "    train_loss = []\n",
        "    train_mae = []\n",
        "\n",
        "    for imgs, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        imgs, labels = imgs.to(device), labels.to(torch.float32).to(device)\n",
        "        predicts = net(imgs)\n",
        "        loss = criterion(predicts,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        mae = torch.mean(torch.abs(predicts.view(-1) - labels.view(-1)))\n",
        "        train_mae.append(mae.item())\n",
        "\n",
        "    return np.mean(train_loss), np.mean(train_mae)\n",
        "\n",
        "\n",
        "def validate(net, valloader, criterion,device):\n",
        "    net.eval()\n",
        "    val_mae = []\n",
        "    val_loss = []\n",
        "    for img, labels in valloader:\n",
        "        img = img.to(device)\n",
        "        labels = labels.to(torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            predicts = net(img)\n",
        "            loss = criterion(predicts, labels)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        mae = torch.mean(torch.abs(labels.view(-1) - predicts.view(-1)))\n",
        "        val_mae.append(mae.item())\n",
        "    return np.mean(val_loss), np.mean(val_mae)"
      ],
      "metadata": {
        "id": "KaLHanYKG2yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_args():\n",
        "  def __init__(self, workers = 10, base_lr=0.002, weight_decay=5e-4, step=\"50,75,88,120\", start_epoch=1,\n",
        "               end_epoch = 350, snapshot='./models/checkpoint/snapshot/', tensorboard=\"./models/checkpoint/tensorboard\", resume='',train_path='/content/train', \n",
        "               test_path='/content/test',train_batchsize=32, test_batchsize=32,  input_size=64):\n",
        "    \n",
        "    self.workers = workers\n",
        "    self.base_lr = base_lr\n",
        "    self.weight_decay = weight_decay\n",
        "    self.step = step\n",
        "    self.start_epoch = start_epoch\n",
        "    self.end_epoch = end_epoch\n",
        "    self.snapshot = snapshot\n",
        "    self.tensorboard = tensorboard\n",
        "    self.resume = resume\n",
        "    self.train_path = train_path\n",
        "    self.test_path = test_path\n",
        "    self.train_batchsize = train_batchsize\n",
        "    self.test_batchsize = test_batchsize\n",
        "    self.input_size = input_size"
      ],
      "metadata": {
        "id": "oiehyxsF33NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Data_args()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "net = SSRNet(image_size=64,class_range=360)\n",
        "#net = ONet()\n",
        "net.to(device)\n",
        "\n",
        "dt = datetime.now().strftime(\"%m%d%Y_%H%M\")\n",
        "snapshot_path = os.path.join(args.snapshot, dt)\n",
        "tensorboard_path = os.path.join(args.tensorboard, dt)\n",
        "\n",
        "if not os.path.exists(snapshot_path):\n",
        "    os.makedirs(snapshot_path)\n",
        "\n",
        "if not os.path.exists(tensorboard_path):\n",
        "    os.makedirs(tensorboard_path)\n",
        "logger = Logger(tensorboard_path)\n",
        "# TODO tensorboard --logdir models/logs/09092022_1212\n",
        "\n",
        "\n",
        "# super params\n",
        "step_epoch = [int(x) for x in args.step.split(',')]\n",
        "\n",
        "criterion = torch.nn.L1Loss()\n",
        "loss_name = criterion.__class__.__name__\n",
        "criterion = criterion.to(device)\n",
        "cur_lr = args.base_lr\n",
        "optimizer = torch.optim.Adam(\n",
        "    net.parameters(),\n",
        "    lr=args.base_lr,\n",
        "    weight_decay=args.weight_decay)\n",
        "\n",
        "# dataset\n",
        "train_transform = Compose([\n",
        "    #VeritcalFlip(),\n",
        "    #Rotate(angle_range = (-90,90), prob = 1.0),\n",
        "    Resize((args.input_size,args.input_size)),\n",
        "    Normalize(mean=(127.5,127.5,127.5),std=(127.5,127.5,127.5)),\n",
        "    #Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ToTensor()\n",
        "])\n",
        "test_transform = Compose([\n",
        "    #VeritcalFlip(),\n",
        "    #Rotate(angle_range=(-90, 90), prob=1.0),\n",
        "    Resize((args.input_size, args.input_size)),\n",
        "    Normalize(mean=(127.5, 127.5, 127.5), std=(127.5, 127.5, 127.5)),\n",
        "    #Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "train_datasets = AngleDatasets(args.train_path,transforms=train_transform)\n",
        "trainloader = DataLoader(\n",
        "    train_datasets,\n",
        "    batch_size=args.train_batchsize,\n",
        "    shuffle=True,\n",
        "    num_workers=args.workers,\n",
        "    drop_last=True)\n",
        "\n",
        "test_datasets = AngleDatasets(args.test_path,transforms=test_transform)\n",
        "valloader = DataLoader(\n",
        "    test_datasets,\n",
        "    batch_size=args.test_batchsize,\n",
        "    shuffle=False,\n",
        "    num_workers=args.workers,\n",
        "    drop_last=True)\n",
        "\n",
        "step_index = 0\n",
        "\n",
        "for epoch in range(args.start_epoch, args.end_epoch + 1):\n",
        "  train_loss, train_MAE = train(net, trainloader,\n",
        "                          criterion, optimizer, device)\n",
        "  val_loss, val_MAE = validate(net,valloader,criterion,device)\n",
        "  if epoch in step_epoch:\n",
        "      step_index += 1\n",
        "      cur_lr = adjust_learning_rate(optimizer, args.base_lr, step_index)\n",
        "\n",
        "  print('Epoch: %d,  train_loss:%6.4f, val_loss:%8.6f, train_MAE:%6.4f, val_MAE:%8.6f, lr:%8.6f' % (epoch, train_loss, val_loss, train_MAE, val_MAE, cur_lr))\n",
        "  filename = os.path.join(\n",
        "      str(snapshot_path), \"checkpoint_epoch_\" + str(epoch) + '.pth')\n",
        "  save_checkpoint(net.state_dict(), filename)\n",
        "\n",
        "  info = {\n",
        "      \"{}/Train\".format(loss_name): train_loss,\n",
        "      \"{}/Val\".format(loss_name): val_loss,\n",
        "\n",
        "      'MSE/Train': train_MAE,\n",
        "      'MSE/Val': val_MAE,\n",
        "\n",
        "      \"Learning rate\": cur_lr\n",
        "  }\n",
        "\n",
        "  for tag, value in info.items():\n",
        "      logger.scalar_summary(tag, value, epoch)"
      ],
      "metadata": {
        "id": "hE22mKZ1G21n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9665a55c-771b-4c93-c3c9-be8fd09a3683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,  train_loss:35.0345, val_loss:8.754964, train_MAE:35.0345, val_MAE:8.754964, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_1.pth\n",
            "Epoch: 2,  train_loss:13.4185, val_loss:16.338616, train_MAE:13.4185, val_MAE:16.338616, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_2.pth\n",
            "Epoch: 3,  train_loss:12.7655, val_loss:6.496736, train_MAE:12.7655, val_MAE:6.496736, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_3.pth\n",
            "Epoch: 4,  train_loss:10.4367, val_loss:4.593840, train_MAE:10.4367, val_MAE:4.593840, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_4.pth\n",
            "Epoch: 5,  train_loss:10.4038, val_loss:6.323503, train_MAE:10.4038, val_MAE:6.323503, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_5.pth\n",
            "Epoch: 6,  train_loss:9.9207, val_loss:7.234535, train_MAE:9.9207, val_MAE:7.234535, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_6.pth\n",
            "Epoch: 7,  train_loss:9.6751, val_loss:10.757370, train_MAE:9.6751, val_MAE:10.757370, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_7.pth\n",
            "Epoch: 8,  train_loss:9.7039, val_loss:11.010611, train_MAE:9.7039, val_MAE:11.010611, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_8.pth\n",
            "Epoch: 9,  train_loss:8.8981, val_loss:4.435704, train_MAE:8.8981, val_MAE:4.435704, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_9.pth\n",
            "Epoch: 10,  train_loss:8.2946, val_loss:3.451378, train_MAE:8.2946, val_MAE:3.451378, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_10.pth\n",
            "Epoch: 11,  train_loss:7.1524, val_loss:6.799520, train_MAE:7.1524, val_MAE:6.799520, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_11.pth\n",
            "Epoch: 12,  train_loss:8.1483, val_loss:4.148692, train_MAE:8.1483, val_MAE:4.148692, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_12.pth\n",
            "Epoch: 13,  train_loss:8.6647, val_loss:9.503443, train_MAE:8.6647, val_MAE:9.503443, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_13.pth\n",
            "Epoch: 14,  train_loss:8.7835, val_loss:12.052493, train_MAE:8.7835, val_MAE:12.052493, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_14.pth\n",
            "Epoch: 15,  train_loss:7.6778, val_loss:9.522032, train_MAE:7.6778, val_MAE:9.522032, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_15.pth\n",
            "Epoch: 16,  train_loss:7.8000, val_loss:3.185514, train_MAE:7.8000, val_MAE:3.185514, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_16.pth\n",
            "Epoch: 17,  train_loss:7.0833, val_loss:2.692121, train_MAE:7.0833, val_MAE:2.692121, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_17.pth\n",
            "Epoch: 18,  train_loss:8.5093, val_loss:6.577316, train_MAE:8.5093, val_MAE:6.577316, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_18.pth\n",
            "Epoch: 19,  train_loss:8.7255, val_loss:3.473404, train_MAE:8.7255, val_MAE:3.473404, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_19.pth\n",
            "Epoch: 20,  train_loss:8.4711, val_loss:5.050664, train_MAE:8.4711, val_MAE:5.050664, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_20.pth\n",
            "Epoch: 21,  train_loss:9.0101, val_loss:5.113103, train_MAE:9.0101, val_MAE:5.113103, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_21.pth\n",
            "Epoch: 22,  train_loss:7.7339, val_loss:3.937257, train_MAE:7.7339, val_MAE:3.937257, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_22.pth\n",
            "Epoch: 23,  train_loss:8.7716, val_loss:8.184991, train_MAE:8.7716, val_MAE:8.184991, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_23.pth\n",
            "Epoch: 24,  train_loss:8.2232, val_loss:2.872566, train_MAE:8.2232, val_MAE:2.872566, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_24.pth\n",
            "Epoch: 25,  train_loss:9.8061, val_loss:4.143743, train_MAE:9.8061, val_MAE:4.143743, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_25.pth\n",
            "Epoch: 26,  train_loss:7.6242, val_loss:4.000127, train_MAE:7.6242, val_MAE:4.000127, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_26.pth\n",
            "Epoch: 27,  train_loss:7.4730, val_loss:9.431577, train_MAE:7.4730, val_MAE:9.431577, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_27.pth\n",
            "Epoch: 28,  train_loss:8.1577, val_loss:2.571466, train_MAE:8.1577, val_MAE:2.571466, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_28.pth\n",
            "Epoch: 29,  train_loss:7.1437, val_loss:4.240886, train_MAE:7.1437, val_MAE:4.240886, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_29.pth\n",
            "Epoch: 30,  train_loss:7.7969, val_loss:2.824396, train_MAE:7.7969, val_MAE:2.824396, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_30.pth\n",
            "Epoch: 31,  train_loss:6.8523, val_loss:7.463089, train_MAE:6.8523, val_MAE:7.463089, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_31.pth\n",
            "Epoch: 32,  train_loss:7.5004, val_loss:2.647399, train_MAE:7.5004, val_MAE:2.647399, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_32.pth\n",
            "Epoch: 33,  train_loss:6.9226, val_loss:6.266375, train_MAE:6.9226, val_MAE:6.266375, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_33.pth\n",
            "Epoch: 34,  train_loss:8.0251, val_loss:3.279714, train_MAE:8.0251, val_MAE:3.279714, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_34.pth\n",
            "Epoch: 35,  train_loss:6.8001, val_loss:2.493182, train_MAE:6.8001, val_MAE:2.493182, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_35.pth\n",
            "Epoch: 36,  train_loss:7.3118, val_loss:2.747398, train_MAE:7.3118, val_MAE:2.747398, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_36.pth\n",
            "Epoch: 37,  train_loss:7.7551, val_loss:5.022883, train_MAE:7.7551, val_MAE:5.022883, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_37.pth\n",
            "Epoch: 38,  train_loss:7.9808, val_loss:2.502278, train_MAE:7.9808, val_MAE:2.502278, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_38.pth\n",
            "Epoch: 39,  train_loss:7.1143, val_loss:2.714534, train_MAE:7.1143, val_MAE:2.714534, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_39.pth\n",
            "Epoch: 40,  train_loss:6.9098, val_loss:4.031909, train_MAE:6.9098, val_MAE:4.031909, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_40.pth\n",
            "Epoch: 41,  train_loss:7.4578, val_loss:7.263483, train_MAE:7.4578, val_MAE:7.263483, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_41.pth\n",
            "Epoch: 42,  train_loss:7.3088, val_loss:8.999805, train_MAE:7.3088, val_MAE:8.999805, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_42.pth\n",
            "Epoch: 43,  train_loss:6.9915, val_loss:2.435946, train_MAE:6.9915, val_MAE:2.435946, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_43.pth\n",
            "Epoch: 44,  train_loss:6.9426, val_loss:2.897859, train_MAE:6.9426, val_MAE:2.897859, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_44.pth\n",
            "Epoch: 45,  train_loss:7.5819, val_loss:2.600582, train_MAE:7.5819, val_MAE:2.600582, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_45.pth\n",
            "Epoch: 46,  train_loss:6.7980, val_loss:2.341486, train_MAE:6.7980, val_MAE:2.341486, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_46.pth\n",
            "Epoch: 47,  train_loss:6.6027, val_loss:2.503291, train_MAE:6.6027, val_MAE:2.503291, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_47.pth\n",
            "Epoch: 48,  train_loss:6.8841, val_loss:3.061063, train_MAE:6.8841, val_MAE:3.061063, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_48.pth\n",
            "Epoch: 49,  train_loss:7.0141, val_loss:2.926374, train_MAE:7.0141, val_MAE:2.926374, lr:0.002000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_49.pth\n",
            "Epoch: 50,  train_loss:7.4635, val_loss:13.516120, train_MAE:7.4635, val_MAE:13.516120, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_50.pth\n",
            "Epoch: 51,  train_loss:6.7017, val_loss:2.274450, train_MAE:6.7017, val_MAE:2.274450, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_51.pth\n",
            "Epoch: 52,  train_loss:5.9904, val_loss:3.795206, train_MAE:5.9904, val_MAE:3.795206, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_52.pth\n",
            "Epoch: 53,  train_loss:5.8899, val_loss:2.480492, train_MAE:5.8899, val_MAE:2.480492, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_53.pth\n",
            "Epoch: 54,  train_loss:5.8876, val_loss:2.248020, train_MAE:5.8876, val_MAE:2.248020, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_54.pth\n",
            "Epoch: 55,  train_loss:5.9165, val_loss:2.089531, train_MAE:5.9165, val_MAE:2.089531, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_55.pth\n",
            "Epoch: 56,  train_loss:5.8629, val_loss:2.050129, train_MAE:5.8629, val_MAE:2.050129, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_56.pth\n",
            "Epoch: 57,  train_loss:5.9127, val_loss:2.895031, train_MAE:5.9127, val_MAE:2.895031, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_57.pth\n",
            "Epoch: 58,  train_loss:5.5957, val_loss:2.360618, train_MAE:5.5957, val_MAE:2.360618, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_58.pth\n",
            "Epoch: 59,  train_loss:5.7645, val_loss:2.743090, train_MAE:5.7645, val_MAE:2.743090, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_59.pth\n",
            "Epoch: 60,  train_loss:5.8471, val_loss:2.060674, train_MAE:5.8471, val_MAE:2.060674, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_60.pth\n",
            "Epoch: 61,  train_loss:5.7885, val_loss:2.313808, train_MAE:5.7885, val_MAE:2.313808, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_61.pth\n",
            "Epoch: 62,  train_loss:5.8608, val_loss:2.334645, train_MAE:5.8608, val_MAE:2.334645, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_62.pth\n",
            "Epoch: 63,  train_loss:5.6576, val_loss:2.253528, train_MAE:5.6576, val_MAE:2.253528, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_63.pth\n",
            "Epoch: 64,  train_loss:5.7878, val_loss:2.395253, train_MAE:5.7878, val_MAE:2.395253, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_64.pth\n",
            "Epoch: 65,  train_loss:5.8121, val_loss:1.961227, train_MAE:5.8121, val_MAE:1.961227, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_65.pth\n",
            "Epoch: 66,  train_loss:5.8613, val_loss:1.947244, train_MAE:5.8613, val_MAE:1.947244, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_66.pth\n",
            "Epoch: 67,  train_loss:5.7688, val_loss:2.386009, train_MAE:5.7688, val_MAE:2.386009, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_67.pth\n",
            "Epoch: 68,  train_loss:5.6701, val_loss:1.975532, train_MAE:5.6701, val_MAE:1.975532, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_68.pth\n",
            "Epoch: 69,  train_loss:5.7553, val_loss:2.575886, train_MAE:5.7553, val_MAE:2.575886, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_69.pth\n",
            "Epoch: 70,  train_loss:5.9707, val_loss:2.458689, train_MAE:5.9707, val_MAE:2.458689, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_70.pth\n",
            "Epoch: 71,  train_loss:5.8507, val_loss:2.365096, train_MAE:5.8507, val_MAE:2.365096, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_71.pth\n",
            "Epoch: 72,  train_loss:5.8141, val_loss:1.974224, train_MAE:5.8141, val_MAE:1.974224, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_72.pth\n",
            "Epoch: 73,  train_loss:5.8572, val_loss:3.142925, train_MAE:5.8572, val_MAE:3.142925, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_73.pth\n",
            "Epoch: 74,  train_loss:5.7047, val_loss:5.017143, train_MAE:5.7047, val_MAE:5.017143, lr:0.000200\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_74.pth\n",
            "Epoch: 75,  train_loss:5.8519, val_loss:4.202419, train_MAE:5.8519, val_MAE:4.202419, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_75.pth\n",
            "Epoch: 76,  train_loss:5.6611, val_loss:2.012608, train_MAE:5.6611, val_MAE:2.012608, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_76.pth\n",
            "Epoch: 77,  train_loss:5.5574, val_loss:1.912963, train_MAE:5.5574, val_MAE:1.912963, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_77.pth\n",
            "Epoch: 78,  train_loss:5.4218, val_loss:1.906293, train_MAE:5.4218, val_MAE:1.906293, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_78.pth\n",
            "Epoch: 79,  train_loss:5.6178, val_loss:1.858320, train_MAE:5.6178, val_MAE:1.858320, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_79.pth\n",
            "Epoch: 80,  train_loss:5.5788, val_loss:1.836843, train_MAE:5.5788, val_MAE:1.836843, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_80.pth\n",
            "Epoch: 81,  train_loss:5.6773, val_loss:1.899448, train_MAE:5.6773, val_MAE:1.899448, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_81.pth\n",
            "Epoch: 82,  train_loss:5.6052, val_loss:1.841989, train_MAE:5.6052, val_MAE:1.841989, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_82.pth\n",
            "Epoch: 83,  train_loss:5.6164, val_loss:1.850596, train_MAE:5.6164, val_MAE:1.850596, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_83.pth\n",
            "Epoch: 84,  train_loss:5.7168, val_loss:1.862668, train_MAE:5.7168, val_MAE:1.862668, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_84.pth\n",
            "Epoch: 85,  train_loss:5.6101, val_loss:1.835064, train_MAE:5.6101, val_MAE:1.835064, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_85.pth\n",
            "Epoch: 86,  train_loss:5.4810, val_loss:1.846198, train_MAE:5.4810, val_MAE:1.846198, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_86.pth\n",
            "Epoch: 87,  train_loss:5.6895, val_loss:1.855138, train_MAE:5.6895, val_MAE:1.855138, lr:0.000020\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_87.pth\n",
            "Epoch: 88,  train_loss:5.7419, val_loss:1.894565, train_MAE:5.7419, val_MAE:1.894565, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_88.pth\n",
            "Epoch: 89,  train_loss:5.5256, val_loss:1.845857, train_MAE:5.5256, val_MAE:1.845857, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_89.pth\n",
            "Epoch: 90,  train_loss:5.6011, val_loss:1.881003, train_MAE:5.6011, val_MAE:1.881003, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_90.pth\n",
            "Epoch: 91,  train_loss:5.5700, val_loss:1.898693, train_MAE:5.5700, val_MAE:1.898693, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_91.pth\n",
            "Epoch: 92,  train_loss:5.5235, val_loss:1.888675, train_MAE:5.5235, val_MAE:1.888675, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_92.pth\n",
            "Epoch: 93,  train_loss:5.7162, val_loss:1.940571, train_MAE:5.7162, val_MAE:1.940571, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_93.pth\n",
            "Epoch: 94,  train_loss:5.5929, val_loss:1.875367, train_MAE:5.5929, val_MAE:1.875367, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_94.pth\n",
            "Epoch: 95,  train_loss:5.6958, val_loss:1.916661, train_MAE:5.6958, val_MAE:1.916661, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_95.pth\n",
            "Epoch: 96,  train_loss:5.6338, val_loss:1.914074, train_MAE:5.6338, val_MAE:1.914074, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_96.pth\n",
            "Epoch: 97,  train_loss:5.7788, val_loss:1.872554, train_MAE:5.7788, val_MAE:1.872554, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_97.pth\n",
            "Epoch: 98,  train_loss:5.6163, val_loss:1.831869, train_MAE:5.6163, val_MAE:1.831869, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_98.pth\n",
            "Epoch: 99,  train_loss:5.5595, val_loss:1.833709, train_MAE:5.5595, val_MAE:1.833709, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_99.pth\n",
            "Epoch: 100,  train_loss:5.4954, val_loss:1.866897, train_MAE:5.4954, val_MAE:1.866897, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_100.pth\n",
            "Epoch: 101,  train_loss:5.5332, val_loss:1.867202, train_MAE:5.5332, val_MAE:1.867202, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_101.pth\n",
            "Epoch: 102,  train_loss:5.6414, val_loss:1.824826, train_MAE:5.6414, val_MAE:1.824826, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_102.pth\n",
            "Epoch: 103,  train_loss:5.7070, val_loss:1.884753, train_MAE:5.7070, val_MAE:1.884753, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_103.pth\n",
            "Epoch: 104,  train_loss:5.5818, val_loss:1.872783, train_MAE:5.5818, val_MAE:1.872783, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_104.pth\n",
            "Epoch: 105,  train_loss:5.7301, val_loss:1.863759, train_MAE:5.7301, val_MAE:1.863759, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_105.pth\n",
            "Epoch: 106,  train_loss:5.6211, val_loss:1.817387, train_MAE:5.6211, val_MAE:1.817387, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_106.pth\n",
            "Epoch: 107,  train_loss:5.4906, val_loss:1.863278, train_MAE:5.4906, val_MAE:1.863278, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_107.pth\n",
            "Epoch: 108,  train_loss:5.4067, val_loss:1.860060, train_MAE:5.4067, val_MAE:1.860060, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_108.pth\n",
            "Epoch: 109,  train_loss:5.7009, val_loss:1.884428, train_MAE:5.7009, val_MAE:1.884428, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_109.pth\n",
            "Epoch: 110,  train_loss:5.6792, val_loss:1.881715, train_MAE:5.6792, val_MAE:1.881715, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_110.pth\n",
            "Epoch: 111,  train_loss:5.6672, val_loss:1.892742, train_MAE:5.6672, val_MAE:1.892742, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_111.pth\n",
            "Epoch: 112,  train_loss:5.7340, val_loss:1.831728, train_MAE:5.7340, val_MAE:1.831728, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_112.pth\n",
            "Epoch: 113,  train_loss:5.7417, val_loss:1.844001, train_MAE:5.7417, val_MAE:1.844001, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_113.pth\n",
            "Epoch: 114,  train_loss:5.6190, val_loss:1.837062, train_MAE:5.6190, val_MAE:1.837062, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_114.pth\n",
            "Epoch: 115,  train_loss:5.5707, val_loss:1.835319, train_MAE:5.5707, val_MAE:1.835319, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_115.pth\n",
            "Epoch: 116,  train_loss:5.6801, val_loss:1.899967, train_MAE:5.6801, val_MAE:1.899967, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_116.pth\n",
            "Epoch: 117,  train_loss:5.6154, val_loss:1.838333, train_MAE:5.6154, val_MAE:1.838333, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_117.pth\n",
            "Epoch: 118,  train_loss:5.6220, val_loss:1.870671, train_MAE:5.6220, val_MAE:1.870671, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_118.pth\n",
            "Epoch: 119,  train_loss:5.3896, val_loss:1.824891, train_MAE:5.3896, val_MAE:1.824891, lr:0.000002\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_119.pth\n",
            "Epoch: 120,  train_loss:5.6764, val_loss:1.831160, train_MAE:5.6764, val_MAE:1.831160, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_120.pth\n",
            "Epoch: 121,  train_loss:5.5216, val_loss:1.838760, train_MAE:5.5216, val_MAE:1.838760, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_121.pth\n",
            "Epoch: 122,  train_loss:5.7417, val_loss:1.841467, train_MAE:5.7417, val_MAE:1.841467, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_122.pth\n",
            "Epoch: 123,  train_loss:5.6823, val_loss:1.845142, train_MAE:5.6823, val_MAE:1.845142, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_123.pth\n",
            "Epoch: 124,  train_loss:5.5376, val_loss:1.851230, train_MAE:5.5376, val_MAE:1.851230, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_124.pth\n",
            "Epoch: 125,  train_loss:5.5711, val_loss:1.858857, train_MAE:5.5711, val_MAE:1.858857, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_125.pth\n",
            "Epoch: 126,  train_loss:5.7680, val_loss:1.841419, train_MAE:5.7680, val_MAE:1.841419, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_126.pth\n",
            "Epoch: 127,  train_loss:5.6449, val_loss:1.841187, train_MAE:5.6449, val_MAE:1.841187, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_127.pth\n",
            "Epoch: 128,  train_loss:5.5442, val_loss:1.834440, train_MAE:5.5442, val_MAE:1.834440, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_128.pth\n",
            "Epoch: 129,  train_loss:5.5243, val_loss:1.838650, train_MAE:5.5243, val_MAE:1.838650, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_129.pth\n",
            "Epoch: 130,  train_loss:5.6424, val_loss:1.849750, train_MAE:5.6424, val_MAE:1.849750, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_130.pth\n",
            "Epoch: 131,  train_loss:5.6147, val_loss:1.849965, train_MAE:5.6147, val_MAE:1.849965, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_131.pth\n",
            "Epoch: 132,  train_loss:5.5397, val_loss:1.846421, train_MAE:5.5397, val_MAE:1.846421, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_132.pth\n",
            "Epoch: 133,  train_loss:5.5363, val_loss:1.838323, train_MAE:5.5363, val_MAE:1.838323, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_133.pth\n",
            "Epoch: 134,  train_loss:5.6554, val_loss:1.839242, train_MAE:5.6554, val_MAE:1.839242, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_134.pth\n",
            "Epoch: 135,  train_loss:5.6428, val_loss:1.842471, train_MAE:5.6428, val_MAE:1.842471, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_135.pth\n",
            "Epoch: 136,  train_loss:5.5848, val_loss:1.837019, train_MAE:5.5848, val_MAE:1.837019, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_136.pth\n",
            "Epoch: 137,  train_loss:5.5089, val_loss:1.833349, train_MAE:5.5089, val_MAE:1.833349, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_137.pth\n",
            "Epoch: 138,  train_loss:5.6168, val_loss:1.842627, train_MAE:5.6168, val_MAE:1.842627, lr:0.000000\n",
            "Save checkpoint to ./models/checkpoint/snapshot/01172023_1150/checkpoint_epoch_138.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-53eee403dbd9>\", line 73, in <module>\n",
            "    train_loss, train_MAE = train(net, trainloader,\n",
            "  File \"<ipython-input-10-0f6214c3531c>\", line 18, in train\n",
            "    for imgs, labels in trainloader:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1282, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1120, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 107, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 424, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.8/inspect.py\", line 744, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validate(net,valloader,criterion,device))"
      ],
      "metadata": {
        "id": "b7u-YrLf34vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153d510d-fd1d-4c09-cd50-123fa0c1c1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1.8441887230708682, 1.8441887230708682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3afjihz234xt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}